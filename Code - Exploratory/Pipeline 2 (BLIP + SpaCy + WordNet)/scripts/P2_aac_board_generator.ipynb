{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5538207",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dynamic AAC Board Generator\n",
    "Combines computer vision and NLP to generate context-appropriate AAC boards.\n",
    "\n",
    "Authors: Kaitlin Moore & Matthew Yurkunas\n",
    "Course: 95-891 Introduction to Artificial Intelligence\n",
    "Date: December 2025\n",
    "\n",
    "Usage:\n",
    "    streamlit run aac_board_generator.py\n",
    "'''\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from gtts import gTTS\n",
    "from nltk.corpus import wordnet\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow messages.\n",
    "\n",
    "BLIP_MODEL_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "\n",
    "# Update this path to your ARASAAC data file location.\n",
    "current_directory = Path.cwd()\n",
    "parent_directory = current_directory.parent\n",
    "ARASAAC_DATA_PATH = f'{parent_directory}/data/arasaac_pictograms_complete_20251106_130529.json'\n",
    "\n",
    "\n",
    "# ARASAAC Symbol Matching\n",
    "class ArasaacMatcher:\n",
    "    '''Handles all ARASAAC symbol matching operations.'''\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        '''Load ARASAAC data and build indexes.'''\n",
    "\n",
    "        print(f'Loading ARASAAC data from {data_path}...')\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.symbols = json.load(f)\n",
    "        print(f'Loaded {len(self.symbols)} symbols\\n')\n",
    "\n",
    "        self.synset_to_symbols = self._build_synset_index()\n",
    "        self.keyword_to_symbols = self._build_keyword_index()\n",
    "\n",
    "    def _build_synset_index(self):\n",
    "        '''Build index mapping synset IDs to symbol IDs.'''\n",
    "\n",
    "        index = defaultdict(list)\n",
    "        for symbol in self.symbols:\n",
    "            for synset in symbol.get('synsets', []):\n",
    "                index[synset].append(symbol['_id'])\n",
    "        return dict(index)\n",
    "\n",
    "    def _build_keyword_index(self):\n",
    "        '''Build index mapping keywords to symbol IDs.'''\n",
    "\n",
    "        index = defaultdict(list)\n",
    "        for symbol in self.symbols:\n",
    "            for kw_obj in symbol.get('keywords', []):\n",
    "                keyword = kw_obj.get('keyword', '').lower()\n",
    "                if keyword:\n",
    "                    index[keyword].append(symbol['_id'])\n",
    "                    plural = kw_obj.get('plural', '').lower()\n",
    "                    if plural:\n",
    "                        index[plural].append(symbol['_id'])\n",
    "        return dict(index)\n",
    "\n",
    "    def get_symbol_metadata(self, symbol_id):\n",
    "        '''Get full metadata for a symbol by ID.'''\n",
    "\n",
    "        for symbol in self.symbols:\n",
    "            if symbol['_id'] == symbol_id:\n",
    "                symbol['image_url'] = (\n",
    "                    f'https://static.arasaac.org/pictograms/'\n",
    "                    f'{symbol_id}/{symbol_id}_500.png'\n",
    "                )\n",
    "                symbol['keyword_list'] = [\n",
    "                    kw['keyword'] for kw in symbol.get('keywords', [])\n",
    "                ]\n",
    "                return symbol\n",
    "        return None\n",
    "\n",
    "    def search_compound(self, compound):\n",
    "        '''Search for compound noun symbols.'''\n",
    "\n",
    "        # Try exact match first (with underscores or spaces).\n",
    "        exact_matches = []\n",
    "\n",
    "        for symbol in self.symbols:\n",
    "            keywords = [\n",
    "                kw.get('keyword', '').lower() for kw in symbol.get('keywords', [])\n",
    "            ]\n",
    "\n",
    "            # Check if compound appears in keywords.\n",
    "            if compound in keywords or compound.replace(' ', '_') in keywords:\n",
    "                exact_matches.append(symbol['_id'])\n",
    "\n",
    "        if exact_matches:\n",
    "            return exact_matches\n",
    "\n",
    "        # Fallback: search for symbols containing both words.\n",
    "        words = compound.split()\n",
    "        matching_symbols = defaultdict(int)\n",
    "\n",
    "        for word in words:\n",
    "            word_symbols = self.keyword_to_symbols.get(word, [])\n",
    "            for sid in word_symbols:\n",
    "                matching_symbols[sid] += 1\n",
    "\n",
    "        # Return symbols that match multiple words.\n",
    "        return [sid for sid, count in matching_symbols.items() if count >= 2]\n",
    "\n",
    "\n",
    "# Image Captioning\n",
    "class SceneCaptioner:\n",
    "    '''Generate captions from images using BLIP.'''\n",
    "\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        '''Initialize BLIP model.'''\n",
    "\n",
    "        print('Loading BLIP model...')\n",
    "        self.device = device\n",
    "        self.processor = BlipProcessor.from_pretrained(BLIP_MODEL_NAME, use_fast=False)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\n",
    "            BLIP_MODEL_NAME,\n",
    "            torch_dtype=torch.float16 if device == 'cuda' else torch.float32\n",
    "        ).to(device)\n",
    "        print('BLIP loaded\\n')\n",
    "\n",
    "    def caption_image(self, image_path, prompt=None):\n",
    "        '''Generate detailed caption for AAC board generation.'''\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Prompts to refine output.\n",
    "        prompts = [\n",
    "            'What do you see?:',\n",
    "        ]\n",
    "\n",
    "        captions = []\n",
    "\n",
    "        for p in prompts:\n",
    "            if p:\n",
    "                inputs = self.processor(images=image, text=p, return_tensors='pt').to(\n",
    "                    self.device,\n",
    "                    torch.float16 if self.device == 'cuda' else torch.float32\n",
    "                )\n",
    "            else:\n",
    "                inputs = self.processor(images=image, return_tensors='pt').to(\n",
    "                    self.device,\n",
    "                    torch.float16 if self.device == 'cuda' else torch.float32\n",
    "                )\n",
    "\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                min_length=60,\n",
    "                num_beams=5,\n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "            caption = self.processor.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )[0]\n",
    "\n",
    "            if caption.startswith(p):\n",
    "                caption = caption[len(p):].strip()\n",
    "\n",
    "            captions.append(caption)\n",
    "\n",
    "        return captions\n",
    "\n",
    "\n",
    "# Concept Extraction\n",
    "class ConceptExtractor:\n",
    "    '''Extract concepts using spaCy NLP.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Load spaCy model.'''\n",
    "\n",
    "        print('Loading spaCy model...')\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        print('spaCy loaded\\n')\n",
    "\n",
    "    def extract_concepts(self, caption):\n",
    "        '''Extract nouns, verbs, adjectives, adverbs, and compounds from text.'''\n",
    "\n",
    "        doc = self.nlp(caption.lower())\n",
    "\n",
    "        concepts = defaultdict(list)\n",
    "\n",
    "        # Extract compounds from noun chunks.\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_text = chunk.text.strip()\n",
    "            words = chunk_text.split()\n",
    "\n",
    "            # Remove leading articles.\n",
    "            if words and words[0] in ['the', 'a', 'an', 'some', 'many']:\n",
    "                words = words[1:]\n",
    "\n",
    "            if len(words) >= 2:\n",
    "                # Check if all tokens in the chunk are nouns.\n",
    "                chunk_tokens = [token for token in chunk if token.text in words]\n",
    "                if all(token.pos_ in ['NOUN', 'PROPN'] for token in chunk_tokens):\n",
    "                    compound = ' '.join(words)\n",
    "                    concepts['compounds'].append(compound)\n",
    "\n",
    "        # Extract individual tokens.\n",
    "        for token in doc:\n",
    "            if token.is_stop or len(token.text) < 3:\n",
    "                continue\n",
    "\n",
    "            lemma = token.lemma_\n",
    "\n",
    "            if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                concepts['nouns'].append(lemma)\n",
    "            elif token.pos_ == 'VERB':\n",
    "                if lemma not in ['be', 'have', 'do', 'will', 'would', 'could', 'should']:\n",
    "                    concepts['verbs'].append(lemma)\n",
    "            elif token.pos_ == 'ADJ':\n",
    "                concepts['adjectives'].append(lemma)\n",
    "            elif token.pos_ == 'ADV':\n",
    "                concepts['adverbs'].append(lemma)\n",
    "\n",
    "        # Remove duplicates while preserving order.\n",
    "        for key in concepts:\n",
    "            concepts[key] = list(dict.fromkeys(concepts[key]))\n",
    "\n",
    "        return dict(concepts)\n",
    "\n",
    "\n",
    "# Synset Mapping\n",
    "class SynsetMapper:\n",
    "    '''Map concepts to WordNet synsets.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Initialize WordNet.'''\n",
    "\n",
    "        print('Loading WordNet...')\n",
    "        self.wn = wordnet\n",
    "        self.pos_map = {\n",
    "            'nouns': wordnet.NOUN,\n",
    "            'verbs': wordnet.VERB,\n",
    "            'adjectives': wordnet.ADJ,\n",
    "            'adverbs': wordnet.ADV\n",
    "        }\n",
    "        print('WordNet loaded\\n')\n",
    "\n",
    "    def get_synsets(self, concept, pos_tag):\n",
    "        '''Get synsets for a concept with given POS.'''\n",
    "\n",
    "        wn_pos = self.pos_map.get(pos_tag, self.wn.NOUN)\n",
    "        synsets = self.wn.synsets(concept, pos=wn_pos)\n",
    "\n",
    "        result = {'names': [], 'offsets': []}\n",
    "\n",
    "        for syn in synsets:\n",
    "            result['names'].append(syn.name())\n",
    "            offset = str(syn.offset()).zfill(8)\n",
    "            pos_suffix = syn.pos()\n",
    "            result['offsets'].append(f'{offset}-{pos_suffix}')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_all_synsets_for_concepts(self, concepts_dict):\n",
    "        '''Get synsets for all concepts in a dictionary.'''\n",
    "\n",
    "        concept_to_synsets = {}\n",
    "\n",
    "        # Handle compounds first.\n",
    "        for compound in concepts_dict.get('compounds', []):\n",
    "            # Try the compound as-is.\n",
    "            synsets = self.get_synsets(compound.replace(' ', '_'), 'nouns')\n",
    "\n",
    "            if synsets['offsets']:\n",
    "                concept_to_synsets[compound] = synsets\n",
    "            else:\n",
    "                # Fallback: try each word separately.\n",
    "                words = compound.split()\n",
    "                combined_synsets = {'names': [], 'offsets': []}\n",
    "                for word in words:\n",
    "                    word_synsets = self.get_synsets(word, 'nouns')\n",
    "                    combined_synsets['names'].extend(word_synsets['names'])\n",
    "                    combined_synsets['offsets'].extend(word_synsets['offsets'])\n",
    "\n",
    "                if combined_synsets['offsets']:\n",
    "                    concept_to_synsets[compound] = combined_synsets\n",
    "\n",
    "        # Then handle regular concepts.\n",
    "        for pos_category, concept_list in concepts_dict.items():\n",
    "            if pos_category == 'compounds':\n",
    "                continue\n",
    "\n",
    "            for concept in concept_list:\n",
    "                synsets = self.get_synsets(concept, pos_category)\n",
    "                if synsets['offsets']:\n",
    "                    concept_to_synsets[concept] = synsets\n",
    "\n",
    "        return concept_to_synsets\n",
    "\n",
    "\n",
    "# Board Generator\n",
    "class BoardGenerator:\n",
    "    '''Complete AAC board generation with WordNet expansion.'''\n",
    "\n",
    "    def __init__(self, arasaac_matcher, captioner=None, extractor=None, synset_mapper=None):\n",
    "        '''Initialize with required components.'''\n",
    "\n",
    "        self.matcher = arasaac_matcher\n",
    "        self.captioner = captioner\n",
    "        self.extractor = extractor\n",
    "        self.synset_mapper = synset_mapper\n",
    "\n",
    "        self.core_vocabulary = [\n",
    "            'i', 'you', 'me', 'they', 'want', 'like', 'need', 'help',\n",
    "            'yes', 'no', 'more', 'stop', 'go', 'come',\n",
    "            'good', 'bad', 'happy', 'sad'\n",
    "        ]\n",
    "\n",
    "    def get_best_symbol(self, symbol_ids, concept):\n",
    "        '''Select the most appropriate symbol from multiple options.'''\n",
    "\n",
    "        if not symbol_ids:\n",
    "            return None\n",
    "\n",
    "        if len(symbol_ids) == 1:\n",
    "            return symbol_ids[0]\n",
    "\n",
    "        best_id = None\n",
    "        best_score = -1\n",
    "\n",
    "        for sid in symbol_ids:\n",
    "            metadata = self.matcher.get_symbol_metadata(sid)\n",
    "            if not metadata:\n",
    "                continue\n",
    "\n",
    "            score = 0\n",
    "            keywords = [kw.lower() for kw in metadata.get('keyword_list', [])]\n",
    "\n",
    "            # Exact match bonus.\n",
    "            if concept.lower() in keywords:\n",
    "                score += 10\n",
    "\n",
    "            # Prefer symbols with fewer keywords (more specific).\n",
    "            score -= len(keywords) * 0.1\n",
    "\n",
    "            # Prefer basic/core categories.\n",
    "            categories = metadata.get('categories', [])\n",
    "            if 'basic' in categories or 'core' in categories:\n",
    "                score += 5\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_id = sid\n",
    "\n",
    "        return best_id if best_id else symbol_ids[0]\n",
    "\n",
    "    def match_concepts(self, concept_to_synsets):\n",
    "        '''Match concepts to ARASAAC symbols via synsets.'''\n",
    "\n",
    "        concept_to_symbols = {}\n",
    "\n",
    "        for concept, synset_data in concept_to_synsets.items():\n",
    "            matched_symbols = set()\n",
    "\n",
    "            # Check if it's a compound (has spaces).\n",
    "            if ' ' in concept:\n",
    "                compound_matches = self.matcher.search_compound(concept)\n",
    "                matched_symbols.update(compound_matches)\n",
    "\n",
    "            # Try synset matching.\n",
    "            for offset in synset_data.get('offsets', []):\n",
    "                symbols = self.matcher.synset_to_symbols.get(offset, [])\n",
    "                matched_symbols.update(symbols)\n",
    "\n",
    "            # Fallback: keyword matching.\n",
    "            if not matched_symbols:\n",
    "                keyword_matches = self.matcher.keyword_to_symbols.get(concept.lower(), [])\n",
    "                matched_symbols.update(keyword_matches)\n",
    "\n",
    "            if matched_symbols:\n",
    "                best_symbol = self.get_best_symbol(list(matched_symbols), concept)\n",
    "                if best_symbol:\n",
    "                    concept_to_symbols[concept] = [best_symbol]\n",
    "\n",
    "        return concept_to_symbols\n",
    "\n",
    "    def match_keywords(self, keywords):\n",
    "        '''Direct keyword matching to ARASAAC symbols.'''\n",
    "\n",
    "        keyword_to_symbols = {}\n",
    "\n",
    "        for keyword in keywords:\n",
    "            matched_symbols = self.matcher.keyword_to_symbols.get(keyword.lower(), [])\n",
    "\n",
    "            if matched_symbols:\n",
    "                best_symbol = self.get_best_symbol(matched_symbols, keyword)\n",
    "                if best_symbol:\n",
    "                    keyword_to_symbols[keyword] = [best_symbol]\n",
    "\n",
    "        return keyword_to_symbols\n",
    "\n",
    "    def expand_via_wordnet(self, image_concepts, current_matches, represented_concepts, min_needed):\n",
    "        '''Expand vocabulary using WordNet hypernyms when below minimum.'''\n",
    "\n",
    "        expanded = {}\n",
    "        added_count = 0\n",
    "\n",
    "        # Collect all original concepts to avoid re-adding them.\n",
    "        original_concepts = set()\n",
    "        for concept_list in image_concepts.values():\n",
    "            if isinstance(concept_list, list):\n",
    "                original_concepts.update([c.lower() for c in concept_list])\n",
    "\n",
    "        # Also track what we've already expanded to avoid duplicates.\n",
    "        already_tried = set(represented_concepts) | original_concepts\n",
    "\n",
    "        # Expand nouns first (most concrete), then verbs.\n",
    "        pos_to_expand = [\n",
    "            ('nouns', wordnet.NOUN),\n",
    "            ('verbs', wordnet.VERB),\n",
    "            ('adjectives', wordnet.ADJ)\n",
    "        ]\n",
    "\n",
    "        for pos_category, wn_pos in pos_to_expand:\n",
    "            if added_count >= min_needed:\n",
    "                break\n",
    "\n",
    "            concepts = image_concepts.get(pos_category, [])\n",
    "            if not isinstance(concepts, list):\n",
    "                continue\n",
    "\n",
    "            for concept in concepts:\n",
    "                if added_count >= min_needed:\n",
    "                    break\n",
    "\n",
    "                synsets = wordnet.synsets(concept, pos=wn_pos)[:2]\n",
    "\n",
    "                for syn in synsets:\n",
    "                    if added_count >= min_needed:\n",
    "                        break\n",
    "\n",
    "                    # Try hypernyms (broader terms: dog -> animal).\n",
    "                    for hypernym in syn.hypernyms()[:3]:\n",
    "                        if added_count >= min_needed:\n",
    "                            break\n",
    "\n",
    "                        word = hypernym.lemmas()[0].name().replace('_', ' ').lower()\n",
    "\n",
    "                        if word in already_tried or len(word) < 3:\n",
    "                            continue\n",
    "\n",
    "                        already_tried.add(word)\n",
    "\n",
    "                        matches = self.matcher.keyword_to_symbols.get(word, [])\n",
    "                        if matches:\n",
    "                            best = self.get_best_symbol(matches, word)\n",
    "                            if best:\n",
    "                                expanded[word] = [best]\n",
    "                                added_count += 1\n",
    "                                print(f\"'    Expanded '{concept}' -> '{word}'\")\n",
    "\n",
    "                    # Try hyponyms (narrower terms: animal -> dog, cat).\n",
    "                    for hyponym in syn.hyponyms()[:2]:\n",
    "                        if added_count >= min_needed:\n",
    "                            break\n",
    "\n",
    "                        word = hyponym.lemmas()[0].name().replace('_', ' ').lower()\n",
    "\n",
    "                        if word in already_tried or len(word) < 3:\n",
    "                            continue\n",
    "\n",
    "                        already_tried.add(word)\n",
    "\n",
    "                        matches = self.matcher.keyword_to_symbols.get(word, [])\n",
    "                        if matches:\n",
    "                            best = self.get_best_symbol(matches, word)\n",
    "                            if best:\n",
    "                                expanded[word] = [best]\n",
    "                                added_count += 1\n",
    "                                print(f\"'    Expanded '{concept}' -> '{word}'\")\n",
    "\n",
    "                    # Try similar words (sister terms via shared hypernym).\n",
    "                    for lemma in syn.lemmas()[:3]:\n",
    "                        if added_count >= min_needed:\n",
    "                            break\n",
    "\n",
    "                        for related in lemma.derivationally_related_forms()[:2]:\n",
    "                            if added_count >= min_needed:\n",
    "                                break\n",
    "\n",
    "                            word = related.name().replace('_', ' ').lower()\n",
    "\n",
    "                            if word in already_tried or len(word) < 3:\n",
    "                                continue\n",
    "\n",
    "                            already_tried.add(word)\n",
    "\n",
    "                            matches = self.matcher.keyword_to_symbols.get(word, [])\n",
    "                            if matches:\n",
    "                                best = self.get_best_symbol(matches, word)\n",
    "                                if best:\n",
    "                                    expanded[word] = [best]\n",
    "                                    added_count += 1\n",
    "                                    print(f\"    Expanded '{concept}' -> '{word}' (related)\")\n",
    "\n",
    "        if expanded:\n",
    "            print(f'  WordNet expansion added {len(expanded)} concepts')\n",
    "\n",
    "        return expanded\n",
    "\n",
    "    def build_board(self, concept_to_symbols, image_concepts, caption='', max_symbols=64, min_symbols=16):\n",
    "        '''Build board with minimum symbol guarantee via WordNet expansion.'''\n",
    "\n",
    "        # Step 1: Map concepts to POS for weighting.\n",
    "        concept_to_pos = {}\n",
    "        for pos_category, concept_list in image_concepts.items():\n",
    "            if isinstance(concept_list, list):\n",
    "                for concept in concept_list:\n",
    "                    concept_to_pos[concept] = pos_category\n",
    "\n",
    "        # Step 2: Score and rank symbols.\n",
    "        symbol_scores = Counter()\n",
    "        symbol_to_concepts = {}\n",
    "\n",
    "        pos_weights = {\n",
    "            'nouns': 1.0,\n",
    "            'verbs': 0.9,\n",
    "            'adjectives': 0.7,\n",
    "            'adverbs': 0.6\n",
    "        }\n",
    "\n",
    "        for concept, symbol_ids in concept_to_symbols.items():\n",
    "            # Base score from frequency.\n",
    "            freq = caption.lower().count(concept.lower()) if caption else 0\n",
    "            pos_category = concept_to_pos.get(concept, 'nouns')\n",
    "            weight = pos_weights.get(pos_category, 0.5)\n",
    "\n",
    "            for symbol_id in symbol_ids:\n",
    "                symbol_scores[symbol_id] += weight + (freq * 0.5)\n",
    "\n",
    "                # Track which concepts this symbol represents.\n",
    "                if symbol_id not in symbol_to_concepts:\n",
    "                    symbol_to_concepts[symbol_id] = []\n",
    "                symbol_to_concepts[symbol_id].append(concept)\n",
    "\n",
    "        # Step 3: Build board with deduplication.\n",
    "        board_symbol_ids = []\n",
    "        represented_concepts = set()\n",
    "\n",
    "        for symbol_id, score in symbol_scores.most_common():\n",
    "            if symbol_id in board_symbol_ids:\n",
    "                continue\n",
    "\n",
    "            symbol_concepts = symbol_to_concepts.get(symbol_id, [])\n",
    "\n",
    "            # Skip if concept already represented.\n",
    "            if any(concept in represented_concepts for concept in symbol_concepts):\n",
    "                continue\n",
    "\n",
    "            board_symbol_ids.append(symbol_id)\n",
    "            represented_concepts.update(symbol_concepts)\n",
    "\n",
    "            if len(board_symbol_ids) >= max_symbols:\n",
    "                break\n",
    "\n",
    "        # Step 4: Expand via WordNet if below minimum.\n",
    "        if len(board_symbol_ids) < min_symbols:\n",
    "            needed = min_symbols - len(board_symbol_ids)\n",
    "            print(f'  Below minimum ({len(board_symbol_ids)}/{min_symbols}), expanding via WordNet...')\n",
    "\n",
    "            expanded_matches = self.expand_via_wordnet(\n",
    "                image_concepts,\n",
    "                concept_to_symbols,\n",
    "                represented_concepts,\n",
    "                needed\n",
    "            )\n",
    "\n",
    "            # Add expanded symbols to board.\n",
    "            for concept, symbol_ids in expanded_matches.items():\n",
    "                if len(board_symbol_ids) >= min_symbols:\n",
    "                    break\n",
    "                if concept in represented_concepts:\n",
    "                    continue\n",
    "\n",
    "                for sid in symbol_ids:\n",
    "                    if sid not in board_symbol_ids:\n",
    "                        board_symbol_ids.append(sid)\n",
    "                        represented_concepts.add(concept)\n",
    "                        break\n",
    "\n",
    "        # Step 5: Get full metadata.\n",
    "        board = []\n",
    "        for sid in board_symbol_ids:\n",
    "            metadata = self.matcher.get_symbol_metadata(sid)\n",
    "            if metadata:\n",
    "                board.append(metadata)\n",
    "\n",
    "        print(f'  Final board: {len(board)} symbols (min: {min_symbols}, max: {max_symbols})')\n",
    "\n",
    "        return board\n",
    "\n",
    "    def generate_board(self, image_path, max_symbols=64, min_symbols=16):\n",
    "        '''Complete pipeline: image -> AAC board.'''\n",
    "\n",
    "        print(f'Generating AAC Board for: {image_path}')\n",
    "\n",
    "        # Caption image.\n",
    "        print('\\nGenerating caption...')\n",
    "        captions = self.captioner.caption_image(image_path)\n",
    "        combined_caption = ' '.join(captions) if isinstance(captions, list) else captions\n",
    "        print(f'Caption: {combined_caption[:100]}...')\n",
    "\n",
    "        # Extract concepts.\n",
    "        print('\\nExtracting concepts...')\n",
    "        concepts = self.extractor.extract_concepts(combined_caption)\n",
    "        print(f'  Nouns: {concepts.get('nouns', [])[:5]}')\n",
    "        print(f'  Verbs: {concepts.get('verbs', [])[:5]}')\n",
    "\n",
    "        # Map to synsets.\n",
    "        print('\\nMapping to WordNet synsets...')\n",
    "        concept_to_synsets = self.synset_mapper.get_all_synsets_for_concepts(concepts)\n",
    "        print(f'  Mapped {len(concept_to_synsets)} concepts')\n",
    "\n",
    "        # Match to ARASAAC.\n",
    "        print('\\nMatching to ARASAAC symbols...')\n",
    "        concept_to_symbols = self.match_concepts(concept_to_synsets)\n",
    "        print(f'  Matched {len(concept_to_symbols)} concepts')\n",
    "\n",
    "        # Build board.\n",
    "        print('\\nBuilding board...')\n",
    "        board = self.build_board(\n",
    "            concept_to_symbols,\n",
    "            concepts,\n",
    "            caption=combined_caption,\n",
    "            max_symbols=max_symbols,\n",
    "            min_symbols=min_symbols\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'captions': captions if isinstance(captions, list) else [captions],\n",
    "            'combined_caption': combined_caption,\n",
    "            'concepts': concepts,\n",
    "            'board': board\n",
    "        }\n",
    "\n",
    "    def generate_board_from_text(self, text, max_symbols=64, min_symbols=16):\n",
    "        '''Generate board from text description (no image).'''\n",
    "\n",
    "        print(f'Generating AAC Board from text: \"{text[:50]}...\"')\n",
    "\n",
    "        # Extract concepts directly from text.\n",
    "        print('\\nExtracting concepts...')\n",
    "        concepts = self.extractor.extract_concepts(text)\n",
    "        print(f'  Nouns: {concepts.get('nouns', [])[:5]}')\n",
    "        print(f'  Verbs: {concepts.get('verbs', [])[:5]}')\n",
    "\n",
    "        # Map to synsets.\n",
    "        print('\\nMapping to WordNet synsets...')\n",
    "        concept_to_synsets = self.synset_mapper.get_all_synsets_for_concepts(concepts)\n",
    "        print(f'  Mapped {len(concept_to_synsets)} concepts')\n",
    "\n",
    "        # Match to ARASAAC.\n",
    "        print('\\nMatching to ARASAAC symbols...')\n",
    "        concept_to_symbols = self.match_concepts(concept_to_synsets)\n",
    "        print(f'  Matched {len(concept_to_symbols)} concepts')\n",
    "\n",
    "        # Build board.\n",
    "        print('\\nBuilding board...')\n",
    "        board = self.build_board(\n",
    "            concept_to_symbols,\n",
    "            concepts,\n",
    "            caption=text,\n",
    "            max_symbols=max_symbols,\n",
    "            min_symbols=min_symbols\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'captions': [text],\n",
    "            'combined_caption': text,\n",
    "            'concepts': concepts,\n",
    "            'board': board\n",
    "        }\n",
    "\n",
    "    def display_board(self, result):\n",
    "        '''Pretty print the board to console.'''\n",
    "\n",
    "        print('\\n' + '=' * 80)\n",
    "        print('FINAL AAC BOARD')\n",
    "        print('=' * 80)\n",
    "        print(f'\\nScenario: \\'{result['combined_caption']}\\'\\n')\n",
    "        print(f'Board contains {len(result['board'])} symbols:\\n')\n",
    "\n",
    "        for i, symbol in enumerate(result['board'], 1):\n",
    "            keywords = ', '.join(symbol['keyword_list'][:3])\n",
    "            print(f'{i:2d}. {keywords:40s}')\n",
    "            print(f'    {symbol['image_url']}\\n')\n",
    "\n",
    "\n",
    "# Streamlit Application\n",
    "def run_streamlit_app():\n",
    "    '''Run the Streamlit web application.'''\n",
    "\n",
    "    st.set_page_config(page_title='AAC Board Generator', layout='wide')\n",
    "\n",
    "    # Initialize session state.\n",
    "    if 'result' not in st.session_state:\n",
    "        st.session_state.result = None\n",
    "    if 'uploaded_image' not in st.session_state:\n",
    "        st.session_state.uploaded_image = None\n",
    "\n",
    "    # Cache component loading.\n",
    "    @st.cache_resource\n",
    "    def load_components():\n",
    "        matcher = ArasaacMatcher(str(ARASAAC_DATA_PATH))\n",
    "        captioner = SceneCaptioner()\n",
    "        extractor = ConceptExtractor()\n",
    "        synset_mapper = SynsetMapper()\n",
    "        generator = BoardGenerator(matcher, captioner, extractor, synset_mapper)\n",
    "        return generator\n",
    "\n",
    "    generator = load_components()\n",
    "\n",
    "    # App Title.\n",
    "    st.title('Dynamic AAC Board Generator')\n",
    "    st.write('Generate communication boards from images or text descriptions')\n",
    "\n",
    "    # Sidebar for input selection and image display.\n",
    "    with st.sidebar:\n",
    "        st.header('Controls')\n",
    "\n",
    "        input_type = st.radio('Input Type:', ['Upload Image', 'Text Description'])\n",
    "\n",
    "        # Board size controls.\n",
    "        st.subheader('Board Settings')\n",
    "        min_symbols = st.slider('Minimum symbols:', 8, 32, 16, 4)\n",
    "        max_symbols = st.slider('Maximum symbols:', 24, 64, 48, 4)\n",
    "\n",
    "        # Show uploaded image in sidebar if it exists.\n",
    "        if st.session_state.uploaded_image:\n",
    "            st.image(\n",
    "                st.session_state.uploaded_image,\n",
    "                caption='Current Image',\n",
    "                use_container_width=True\n",
    "            )\n",
    "\n",
    "        # Clear board button.\n",
    "        if st.session_state.result:\n",
    "            if st.button('Clear Board'):\n",
    "                st.session_state.result = None\n",
    "                st.session_state.uploaded_image = None\n",
    "                st.rerun()\n",
    "\n",
    "    # Main content area.\n",
    "    if input_type == 'Upload Image':\n",
    "        uploaded_file = st.file_uploader('Choose an image', type=['jpg', 'jpeg', 'png'])\n",
    "\n",
    "        if uploaded_file:\n",
    "            # Store image in session state.\n",
    "            image = Image.open(uploaded_file)\n",
    "            st.session_state.uploaded_image = image\n",
    "\n",
    "            if st.button('Generate Board', type='primary'):\n",
    "                with st.spinner('Generating AAC board...'):\n",
    "                    # Save temp file.\n",
    "                    image.save('temp_image.jpg')\n",
    "                    st.session_state.result = generator.generate_board(\n",
    "                        'temp_image.jpg',\n",
    "                        max_symbols=max_symbols,\n",
    "                        min_symbols=min_symbols\n",
    "                    )\n",
    "\n",
    "                    st.success(f'Generated {len(st.session_state.result['board'])} symbols!')\n",
    "\n",
    "    else:  # Text Description.\n",
    "        text_input = st.text_area(\n",
    "            'Enter scenario description:',\n",
    "            placeholder='e.g., Children playing at the playground with slides and swings',\n",
    "            height=100\n",
    "        )\n",
    "\n",
    "        if st.button('Generate Board', type='primary') and text_input:\n",
    "            with st.spinner('Generating AAC board...'):\n",
    "                # Use the text-to-board method.\n",
    "                st.session_state.result = generator.generate_board_from_text(\n",
    "                    text_input,\n",
    "                    max_symbols=max_symbols,\n",
    "                    min_symbols=min_symbols\n",
    "                )\n",
    "\n",
    "                st.session_state.uploaded_image = None  # Clear image when using text.\n",
    "                st.success(f'Generated {len(st.session_state.result['board'])} symbols!')\n",
    "\n",
    "    # Display Board.\n",
    "    if st.session_state.result:\n",
    "        result = st.session_state.result\n",
    "\n",
    "        st.markdown('---')\n",
    "        st.subheader('Generated AAC Board')\n",
    "        st.write(f'**Scene:** {result['combined_caption']}')\n",
    "\n",
    "        # Display controls.\n",
    "        num_symbols = st.slider(\n",
    "            'Number of symbols to display:',\n",
    "            8, 64,\n",
    "            min(24, len(result['board'])),\n",
    "            4\n",
    "        )\n",
    "        cols_per_row = st.slider('Symbols per row:', 4, 8, 6, 1)\n",
    "\n",
    "        board = result['board'][:num_symbols]\n",
    "        st.write(f'Displaying {len(board)} of {len(result['board'])} total symbols')\n",
    "\n",
    "        # Render board grid.\n",
    "        for row_start in range(0, len(board), cols_per_row):\n",
    "            cols = st.columns(cols_per_row)\n",
    "\n",
    "            for idx, col in enumerate(cols):\n",
    "                symbol_idx = row_start + idx\n",
    "                if symbol_idx < len(board):\n",
    "                    symbol = board[symbol_idx]\n",
    "\n",
    "                    with col:\n",
    "                        # Display image.\n",
    "                        st.image(symbol['image_url'], use_container_width=True)\n",
    "\n",
    "                        # Keyword label.\n",
    "                        keyword = symbol['keyword_list'][0] if symbol.get('keyword_list') else 'N/A'\n",
    "                        st.markdown(f'**{keyword}**', unsafe_allow_html=True)\n",
    "\n",
    "                        # Audio button.\n",
    "                        if st.button('Play', key=f'audio_{symbol_idx}'):\n",
    "                            try:\n",
    "                                tts = gTTS(text=keyword, lang='en', slow=False)\n",
    "\n",
    "                                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "                                    tts.save(fp.name)\n",
    "                                    st.audio(fp.name, format='audio/mp3', autoplay=True)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                st.warning(f'Could not generate audio for \"{keyword}\"')\n",
    "\n",
    "    else:\n",
    "        # Welcome message when no board is generated.\n",
    "        st.info('Upload an image or enter a text description to generate an AAC board.')\n",
    "\n",
    "\n",
    "# Main Entry Point\n",
    "if __name__ == '__main__':\n",
    "    run_streamlit_app()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
