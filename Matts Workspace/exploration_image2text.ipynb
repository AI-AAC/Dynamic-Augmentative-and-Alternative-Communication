{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db39319",
   "metadata": {},
   "source": [
    "# Image to Text\n",
    "#### The goal is to turn an image into a textual description for further processing in our \"Text to Board\" stage.\n",
    "\n",
    "Older methods include using a CNN/RNN combination (encoder-decoder). The benefits for this method are more control over outputs and it can run on older hardware. However, for a more modern approach, I will be trying BLIP (Bootstrapping Language-Image Pre-training). Specifically, I will be using BLIP-2 following this guide from HuggingFace on how to deploy:\n",
    "https://huggingface.co/blog/blip-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc97514",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
